<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://ziorzis.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ziorzis.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-22T10:20:02+00:00</updated><id>https://ziorzis.github.io/feed.xml</id><title type="html">Yorgos Deligiannidis</title><subtitle>Yorgos Deligiannidis&apos; website. </subtitle><entry><title type="html">Flow Annealed Importance Sampling Bootstrap</title><link href="https://ziorzis.github.io/blog/2024/FAB/" rel="alternate" type="text/html" title="Flow Annealed Importance Sampling Bootstrap"/><published>2024-11-17T00:00:00+00:00</published><updated>2024-11-17T00:00:00+00:00</updated><id>https://ziorzis.github.io/blog/2024/FAB</id><content type="html" xml:base="https://ziorzis.github.io/blog/2024/FAB/"><![CDATA[<p>My notes on <a href="https://arxiv.org/abs/2111.11510">Bootstrap Your Flow</a> and <a href="https://arxiv.org/abs/2208.01893">Flow Annealed Importance Sampling Bootstrap</a> (FAB).</p> <h2 id="tldr">TL;DR</h2> <p>Training parametric distributions \(q_{\theta}\) to approximate a target density \(p\) often suffers from high-variance objectives. These objectives can sometimes be rewritten via an Annealed Importance Sampling (AIS) estimator. Choosing the AIS target distribution in a smart way can help reduce the variance of these objectives.</p> <h2 id="setup">Setup</h2> <p>Suppose we want to approximate some target density \(p\). We assume access to a parametric family of densities \(q_{\theta}\) that we can sample from and evaluate. For example, \(q_{\theta}\) may be parametrized as a normalizing flow. Given some divergence \(D\) between probability distributions, we will select \(\theta\) to minimize the divergence \(D\) between \(q_{\theta}\) and \(p\).</p> <h3 id="kl-example">KL Example</h3> <p>Consider the classical example of the KL divergence \(\text{KL}(q \| p) = \int q(x)\log \frac{q(x)}{p(x)}dx\). We could either try and minimize \(\text{KL}(q_{\theta} \| p)\) or \(\text{KL}(p \| q_{\theta})\). Let’s consider \(\text{KL}(q_{\theta} \| p)\). This objective is tractable to optimize (yay!) since we can sample from \(q_{\theta}\). However, it exhibits mode-seeking behaviour. This is because \(\log \frac{q_{\theta}(x)}{p(x)}\) blows up when \(q_{\theta}\) assigns high density to \(x\) such that \(p(x)\) is small. Hence, minimizing \(\text{KL}(q_{\theta} \| p)\) will choose a \(q_{\theta}\) that only assigns mass where \(p\) has sufficient density. The optimal \(q_{\theta}\) will avoid placing any mass where \(p\) does not have a lot. So, if \(p\) is multi-modal, \(q_{\theta}\) will most likely just learn a few of these modes and place all mass in them. We drop modes as a result. This is a downside of the objective.</p> <p>On the other hand, if we optimize \(\text{KL}(p \| q_{\theta})\), then this exhibits mode-covering behaviour. This is because \(\log \frac{p(x)}{q_{\theta}(x)}\) will blow up when \(p\) assigns a lot of mass to \(x\), but \(q_{\theta}\) doesn’t. So, the learned \(q_{\theta}\) will try and place mass everywhere that \(p\) has mass (yay!). However, the issue is that we now need samples from \(p\) to train with this objective. We might not have access to samples from \(p\) though (or getting samples could be very expensive). So, we need to find other ways of training this objective without relying on samples from \(p\).</p> <p>We could try to use Importance Sampling (IS) and rewrite the objective as \(\mathbb{E}_{x \sim q_{\theta}}[\frac{p(x)}{q_{\theta}(x)}\log \frac{p(x)}{q_{\theta}(x)}]\). We still have the mode-covering behaviour (since the objective itself has not changed) and we can now train on samples from \(q_{\theta}\). However, the variance of our IS estimator will be very large. That is, \(\text{Var}_{x \sim q_{\theta}}(\frac{p(x)}{q_{\theta}(x)}\log \frac{p(x)}{q_{\theta}(x)}))\) can be big and make optimization tricky. In particular, we know that the expected value is positive (since KL is positive). Observe that \(\frac{p(x)}{q_{\theta}(x)}\log \frac{p(x)}{q_{\theta}(x)}\) is positive for \(x\) such that \(p(x) &gt; q_{\theta}(x)\). However, if \(q_{\theta}\) is very different from \(p\) (for example, at the onset of training), then most samples \(x\) from \(q_{\theta}\) will be such that \(q_{\theta}(x) &gt; p(x)\) and thus \(\frac{p(x)}{q_{\theta}(x)}\log \frac{p(x)}{q_{\theta}(x)} &lt; 0\). Said another way, for most \(x \sim q_{\theta}\), the integrand will be negative. To compensate, once in a while the integrand will be a very large positive value. This makes training challenging (have high variance).</p> <h2 id="main-idea-of-fab">Main Idea of FAB</h2> <p>This tradeoff exists for many divergences \(D\), which can often be written in the form \(D(q \| p) = \int q(x) f(q(x), p(x))dx\) for some \(f\). \(D(q \| p)\) will often be mode-seeking, whereas \(D(p \|q)\) will either require samples from \(p\) or have high variance if using a modified IS objective for training. (This is exactly what we saw in the KL example above.)</p> <p>FAB considers the case of a mode-covering \(D(p \|q_{\theta})\). Note that for any density \(g\), we have</p> \[D(p \|q_{\theta}) = \mathbb{E}_{x \sim g}[\frac{p}{g}f(p, q_{\theta})].\] <p>As discussed for the KL example, often, if we set \(g = q_{\theta}\), then the IS estimator has high variance. However, we know that the optimal \(g\) (the one minimizing the variance) is \(g_*(x) \propto p(x)f(p(x), q_{\theta}(x))\). Sampling from this optimal \(g_{*}\) is usually not tractable (which is what we would want for IS). So, FAB suggests using AIS to estimate the above objective. In particular, AIS will be initialized at \(q_{\theta}\) and will target \(g_{*}\). The output of AIS is a collection of samples \(x_1, \dots, x_n\) and corresponding weights \(w_1, \dots , w_n\). If AIS is implemented well, then the particles \(x_i\) will be distributed approximately as \(g_{*}\). Regardless though, AIS guarantees us that the weights \(w_i\) are chosen such that for any function \(h\), we have</p> \[\mathbb{E}_{\text{AIS}}[w_i h(x_i)] = \int h(x) g_{*}(x)dx,\] <p>where the expectation is over the randomness of the AIS sampling procedure. Therefore we can now use the AIS outputted samples and weights to obtain an unbiased estimator of our training objective (i.e. we take \(h = \frac{p}{g_{*}}f(p, q_{\theta})\)). Since the samples are approximately from \(g_{*}\), the variance of our estimator should be small and so training will be tractable!</p> <h2 id="paper-details">Paper Details</h2> <p>Both papers specifically focus on using the \(\alpha\)-divergence with \(\alpha = 2\). Up to constants, this is given by</p> \[D_{2}(p \|q_{\theta}) = \int \frac{p^2(x)}{q_{\theta}(x)}dx.\] <p>This objective is a natural one as it is mode-covering and it corresponds to minimizing the variance of the importance weights \(\frac{p(x)}{q_{\theta}(x)}\). (In particular, after training \(q_{\theta}\), if IS is used to estimate expectations w.r.t. \(p\) so that the estimator is unbiased, then the weights will have low variance.) As discussed, this objective can be estimated with either samples from \(p\) or \(q_{\theta}\). However, the estimator will have lower variance if we use \(p\). So, the original paper Bootstrap Your Flow paper suggests using AIS to target \(p\). On the other hand, the Annealed Importance Sampling Bootstrap paper suggests using AIS to target \(g_{*} \propto \frac{p^2}{q_{\theta}}\) (i.e. choosing \(g\) that will induce the minimal variance estimator).</p> <p>This method is performing a sort of bootstrapping since as the flow \(q_{\theta}\) improves, the initial samples to AIS will be closer to \(p\) which should make the final samples closer to \(\frac{p^2}{q}\) and so the AIS estimator will have lower variance. So, the expected values computed via AIS will be improved, and so the gradient information passed to optimizing \(q_{\theta}\) will be better, and thus \(q_{\theta}\) will become even closer to \(p\).</p> <p>The paper explores using both the original weights \(w_i\) from AIS and the self-normalized weights \(\frac{w_i}{\sum_j w_j}\). They found that using the self-normalized weights improved training. (Just watch out for whether you are differentiating through the weights or not, as they depend implicitly on \(\theta\)).</p> <p>See the papers for extensions on using AIS bootstrapping to train with other divergences/objectives/models.</p> <p>Here’s one idea I have. As the authors point out, the distribution that minimizes the variance of the self-normalized weights is different than for the unnormalized ones. What if AIS was used to target this distribution instead? In general, this would not be tractable. However, for some divergences it will be, and may improve results even more.</p> <h2 id="replay-buffer">Replay Buffer</h2> <p>The next contribution of Annealed Importance Sampling Bootstrap is to introduce a <em> prioritized replay buffer</em>. Once the parameters \(\theta\) are updated, we could dispose with the old AIS samples, and draw entirely new ones from AIS initialized at the new \(q_{\theta}\) targeting the new \(g_{*}\). This would require invoking the AIS procedure often, which may be very expensive. Instead, the authors propose to keep old samples in a buffer and to adjust the training procedure accordingly. Thus we can rely on the AIS sampling procedure less frequently.</p> <p>Suppose that the replay buffer has samples \(x_i\) with corresponding weights \(w_i\) targeting \(g_{\theta'} \propto \frac{p^2}{q_{\theta'}}\). By rescaling the weights via</p> \[w_i \mapsto w_i \frac{g_{\theta}}{g_{\theta'}} = w_i \frac{q_{\theta'}}{q_{\theta}},\] <p>the existing samples \(x_i\) and updated weights \(w_i \frac{q_{\theta'}}{q_{\theta}}\) now target \(g_{\theta} \propto \frac{p^2}{q_{\theta}}\). Therefore, suppose we store the weights, particles and previous \(q_{\theta'}\) values in the buffer. After we update to the new parameters \(\theta\), we can iterate through the buffer to update the weights and \(q_{\theta'}\) values so that they target the new \(g_{\theta}\). In this way, we can keep old samples and add fresh ones to the buffer without breaking anything. It is also nice that we do not need to reevaluate \(p\) which could be very expensive.</p> <p>The authors also propose that instead of sampling uniformly from the buffer and using the AIS weights to weight the expected values, to rather sample proportional to the normalized AIS weights (and not include a weight in the expected value calculation). This procedure has the same expected value as the self-normalizing objective.</p> <p>Since we are sampling in proportion to the weights \(w \approx \frac{g}{q_{\theta}} = \frac{p^2}{q_{\theta}^2}\), this means that sampling prioritizes samples where the current flow \(q_{\theta}\) under assigns mass relative to the target \(p\). As \(q_{\theta}\) improves on these points, the weights of these particles will decrease, and so the buffer will prioritize other samples instead, providing variability.</p> <p>Iterating through all of the data points in the buffer before sampling is expensive, so the paper discusses some approximations to speed up training at the cost of introducing a small bias.</p> <h2 id="todo">TODO</h2> <p>I want to read some more of the related literature before returning to their RELATED WORK and DISCUSSION sections.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Notes on FAB]]></summary></entry><entry><title type="html">Action Matching</title><link href="https://ziorzis.github.io/blog/2024/action-matching/" rel="alternate" type="text/html" title="Action Matching"/><published>2024-11-16T00:00:00+00:00</published><updated>2024-11-16T00:00:00+00:00</updated><id>https://ziorzis.github.io/blog/2024/action-matching</id><content type="html" xml:base="https://ziorzis.github.io/blog/2024/action-matching/"><![CDATA[<p>My notes on <a href="https://arxiv.org/pdf/2210.06662">Action Matching: Learning Stochastic Dynamics from Samples</a>.</p> <h2 id="tldr">TL;DR</h2> <p>Given dynamics/densities \(q_t\), there are many different vector fields \(v_t\) that provide us with ODEs or SDEs to simulate these dynamics. Action Matching proposes to choose \(v_t\) to be the unique gradient field (of the form \(\nabla s_t\)) in each case. The paper provides tractable objectives (if one can sample from \(q_t\)), extensions, and connections to optimal transport.</p> <h2 id="continuity-equation">Continuity Equation</h2> <p>Suppose that we have a continuous path of densities \((q_t)_{t \geq 0}\). We may want a vector field that pushes particles to follow these densities. In particular, suppose that a particle is initialized as \(x_0 \sim q_0\). We can evolve the particle to follow some time-dependent vector field \(v_t\) as follows</p> \[\frac{d}{dt}x(t) = v_t(x(t)), \quad x(0) = x_0.\] <p>The densities \(q_t\) are equal to the laws of \(x(t)\) iff the <em>continuity equation</em> holds:</p> \[\frac{\partial}{\partial t}q_t(x) = - \text{div}(q_t v_t)(x) = - \nabla \cdot (q_t v_t)(x).\] <p>Recall the intuition that \(\text{div}(q_t v_t)(x)\) tells us how much outward flow the density-weighted vector field \(v_t q_t\) induces at a point \(x\), and so the density will change opposite to that.</p> <h2 id="helmholtz-decomposition">Helmholtz Decomposition</h2> <p>The standard <a href="https://en.wikipedia.org/wiki/Helmholtz_decomposition">Helmholtz Decomposition</a> states that a vector field \(v\) can be decomposed as</p> \[\begin{aligned} v(x) &amp;= \nabla s(x) + w(x), \\ \text{div}(w)(x) &amp;= 0. \end{aligned}\] <p>In particular, this means that \(v\) can be written as the sum of a gradient field \(\nabla s\) and a divergence-free term \(w\). Under some conditions, the decomposition is unique (up to a constant for \(s\)).</p> <p>This result can be extended to consider the divergence operator with respect to some density \(q\). In particular, any vector field \(v\) can be uniquely decomposed as</p> \[v(x) = \nabla s(x) + w(x)\] <p>where \(w\) is divergence free w.r.t. \(q\) in the sense that \(\text{div}(qw)(x) = 0\).</p> <h2 id="action-matching">Action Matching</h2> <p>For a given path of densities \(q_t\), there are many vector fields \(v_t\) that will satisfy the continuity equation, and thus keep the dynamics evolving as \(q_t\). In particular, adding any \(a_t\) to \(v_t\) such that \(\text{div}(q_ta_t) = 0\) will still satisfy the continuity equation.</p> <p>However, the Helmholtz Decomposition tells us that all \(v_t\) that satisfy the continuity equation will have the same unique gradient field \(\nabla s_t\). In particular, if \(v^{(1)}_t, v^{(2)}_t\) both satisfy the continuity equation and their Helmholtz Decompositions are \(v^{(i)}_t = \nabla s^{(i)}_t + w^{(i)}_t\), then</p> \[\begin{aligned} 0 &amp;= \frac{\partial}{\partial t}(q_t-q_t) \\ &amp;= \text{div}(q_t(\nabla s^{(1)}_t-\nabla s^{(2)}_t)) + \text{div}(q_tw^{(1)}_t) -\text{div}(q_tw^{(2)}_t) \\ &amp;= \text{div}(q_t(\nabla s^{(1)}_t-\nabla s^{(2)}_t)) + 0 - 0. \end{aligned}\] <p>Hence, the Helmholtz Decomposition is unique only if \(\nabla s^{(1)}_t-\nabla s^{(2)}_t = 0\). Otherwise, we could add and subtract a multiple of \(\nabla s^{(1)}_t-\nabla s^{(2)}_t\) from \(\nabla s^{(i)}_t\) and \(w^{(i)}_t\) respectively, without changing \(v^{(i)}_t\).</p> <p>Therefore Action Matching (AM) suggests choosing \(v_t\) in a canonical way, by setting \(v_t = \nabla s_t\), the unique gradient field satisfying the continuity equation. The scalar function \(s_t\) (unique up to a constant) is called the <em>action</em>.</p> <p>Theorem 2.2 provides a tractable objective for learning an approximation \(\hat{s}_t\) to the true action \(s_t\). Once \(\hat{s}_t\) is trained, sampling can be performed by evolving an ODE as follows:</p> \[\frac{d}{dt}x(t) = \nabla \hat{s}_t(x(t)), \quad x(0) \sim q_0.\] <h2 id="extensions">Extensions</h2> <p>The paper also introduces entropic Action Matching (eAM). Instead of evolving an ODE to satisfy the dynamics \(q_t\), one could also evolve an SDE:</p> \[dX(t) = v_t(X(t))dt + \sigma_t dB_t, \quad X(0) \sim q_0.\] <p>The evolution of such a density is characterized by the Fokker-Planck equation which states that the law of \(X(t)\) has density \(q_t\) iff</p> \[\frac{\partial}{\partial t}q_t = - \text{div}(q_t v_t) + \frac{\sigma_t^2}{2}\Delta q_t.\] <p>The intuition behind \(\frac{\sigma_t^2}{2}\Delta q_t\) is that the Laplacian \(\Delta q_t(x)\) tells us how much more density there is around a point \(x\) than at \(x\). So, if this value is high, the diffusion will increase the amount of density at \(x\).</p> <p>Fixing \(\sigma_t\), there are once again many different \(v_t\) that satisfy the above (we can add any divergence-free term to \(v_t\)). However, eAM suggests choosing \(v_t\) to be the unique scalar field \(v_t = \nabla s_t\) that satisfies the Fokker-Planck equation. Proposition 3.2 provides a tractable objective.</p> <p>The paper also introduces unbalanced Action Matching where particles can be created and destroyed according to some growth rate \(g_t(x)\) for position \(x\) at time \(t\). Such dynamics are governed by the ODE \(\frac{\partial}{\partial t} q_t = - \text{div}(q_t v_t) + q_t g_t\).</p> <p>The paper extends AM to more general convex costs and discusses connections to optimal transport.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Notes on Action Matching]]></summary></entry></feed>